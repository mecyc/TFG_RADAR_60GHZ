\capitulo{3}{Conceptos teóricos}

Para poder construir un modelo capaz de capturar con precisión las características ofrecidas del resultado de leer cada objeto, es necesario comprender el origen y la estructura de la señal recibida. En este apartado se introducen algunos conceptos fundamentales del sistema de radar.

\section{Radar}

El radar o mejor conocido por su acrónimo del inglés, \textit{radio detecting and ranging}, es un sistema electrónico que puede emitir y recibir radiación en un cierto rango del espectro electromagnético, y cuyo procesamiento permite la estimación de distancias, altitudes, direcciones y velocidades de objetos. El sistema de radar tiene su uso muy extendido en los campos de la meteorología, el control del tráfico aéreo y terrestre y múltiples usos militares \cite{AlonsoCerpa}.

Se han desarrollado múltiples avances tecnológicos los cuales han hecho posible que en la actualidad se puedan emplear los radares en diferentes y diversas áreas con seguridad y eficiencia. Los constantes avances tecnológicos en el desarrollo de materiales y su implementación en los circuitos integrados, junto con el desarrollo de la informática, son factores importantes que han convertido a los sistemas de radares en instrumentos de bajo tamaño con una gran precisión y calidad. 

\subsection{Principio de funcionamiento del Radar}

El radar es un sistema de teledetección activa\footnote{Proceso de teledetección que funciona con su propia fuente de emisión o luz.} que emite ondas de microondas, con frecuencias comprendidas entre 1 GHz y 100 GHz. Estos valores también se pueden considerar como longitudes de onda con valores entre los 3 mm y 30 cm \cite{HerreraG2009}. 

Las ondas de microondas tienen diferentes ventajas:
\begin{itemize}
\item Son capaces de atravesar los cúmulos de nubes, la lluvia y la nieve.
\item Permiten trabajar en condiciones de oscuridad debido a que no dependen de la iluminación solar o artificial.
\item Miden características de los materiales como la rugosidad y la humedad, entre otras características.
\end{itemize} 

Los sistemas radar de apertura sintética, también conocidos como SAR, cuentan con un sensor radar capaz de emitir un pulso de ondas de microondas dirigida hacia la superficie del terreno u objeto. En cierto momento ese pulso retorna en dirección al sistema SAR que se encuentra ubicado en un satélite donde la información conseguida es medida por el sensor radar. 

Los pulsos empleados por el sistema SAR, típicamente poseen longitudes de onda que pueden variar desde los 3 cm de la banda \textit{X}, hasta los 24 cm de la banda \textit{L}, por lo que cuanto mayor sea la longitud de onda, mayor será su capacidad de penetración en zonas vegetadas, sin embargo, habría que considerar que será peor su resolución espacial.

Los sistemas SAR pueden ser transportados mediante un satélite, un avión o permanecer en una plataforma terrestre, en donde se emiten y reciben la señal del pulso de microondas emitido por el sensor radar, generando imágenes complejas de alta resolución espacial de la superficie del terreno.

Podrían mencionarse diferentes casos en los cuales se generan imágenes complejas de alta resolución espacial, por ejemplo:
\begin{itemize}
\item El satélite \textit{ERS-1/2} \cite{Wolff2022Dec} junto con él satélite \textit{ENVISAT} (pertenecientes a la Agencia Espacial Europea) poseen un sensor que emite ondas en la banda \textit{C}, esta onda tiene una longitud de onda de 6 cm, que generan imágenes SAR, éstas pueden llegar a tener una resolución espacial de $ 4 m \times 20 m$.
\item El satélite \textit{TERRASAR X} \cite{TerraSAR} es otro ejemplo de sistema SAR, en el que las ondas de microondas son generadas en la denominada \textit{banda X}. Permite obtener imágenes de resoluciones de $1 \times 3 m$.
\end{itemize}


En la obtención de imágenes SAR una de las partes más importantes del radar es el emisor (instrumento que produce la radiación electromagnética). Se puede considerar que cada píxel es representado con un formato de número complejo que contiene un valor de amplitud, junto con otro valor de fase. El valor de la amplitud está directamente relacionado con el coeficiente de respuesta generado con la información de la superficie del terreno dirigido a la señal de respuesta\cite{RigoRibas2004}.

\subsection{Tipos de radar}	

En el mercado y de acuerdo a su forma de operar existen diferentes tipos de radares. Estos pueden operar de modo primario o de modo secundario, considerando que todos los radares tienen la misma base de operabilidad, ya que todos los radares tienen en común la capacidad de poder transmitir una señal, pudiendo generar la evaluación mediante diferentes técnicas de procesamiento de las señales recibidas para así poder obtener el parámetro buscado \cite{AcostaOsorio2014Mar}. 

Los sistemas de radar por lo general se encuentran seccionados en diferentes categorías de operación las cuales están basadas en los diferentes métodos de transmisión de la señal. Por lo general estos métodos corresponden a la transmisión de pulsos, la onda continua y la frecuencia modulada. El método de transmisión de pulsos es el método más común para implementarse en la transmisión de la energía del radar. 

El método de onda continua se basa en el principio del \textit{«El Efecto Doppler»}, este principio es capaz de generar la detección de la presencia y velocidad de un objeto en movimiento en dirección al radar o en dirección opuesta al radar. Suele ser empleado en los sistemas de control de fuego o disparo en el campo militar, para poder rastrear objetos que presentan movimientos rápidos en un rango cercano.


\section{Teledetección}

La teledetección, también conocida como detección a distancia o detección remota, es la técnica que permite recopilar información a distancia de los objetos sin que exista un contacto físico, para ello es necesario tener una interacción entre los objetos a estudiar y un sensor situado en una plataforma.

En la teledetección el radar es un sensor activo de microondas desplazado a bordo de una plataforma explorando la tierra, por lo general se encuentra emitiendo pulsos de energía con dirección a la superficie terrestre, pudiendo almacenar las señales obtenidas de retorno \cite{Marchionni2014}. 

Debido a que tienen que trabajar con haces de energía emitidos artificialmente, los sistemas de radar permiten controlar las condiciones de la adquisición basadas en la frecuencia, la polarización y la geometría de la observación. Pueden recolectar datos en cualquier condición ya sea de día o de noche, el realizar este tipo de trabajo resulta muy ventajoso en las regiones polares, lugares donde los prolongados períodos de oscuridad dificultan la adquisición de imágenes convencionales. 

Un sistema de teledetección por radar transmite diferentes pulsos de microondas que van barriendo la superficie terrestre, la porción de energía que es reflejada o retrodispersada retorna de vuelta hacia el sensor del radar. En el momento que el radar recibe la señal el sistema registra la intensidad de la señal de retorno conocida como radiación retrodispersada, donde el retardo en tiempo, entre la transmisión y recepción de cada pulso de energía, es el tiempo que se relaciona con la distancia de los objetos observados.

La distancia a la cual debe estar situado un sensor para poder ser considerado remoto puede variar ya sea desde pocos decímetros hasta miles de kilómetros.

La teledetección es un flujo de radiación emitido por los objetos o materiales hacia un radar o sensor. El origen del flujo puede venir de distintas ubicaciones:

\begin{itemize}
\item[•] Radiación emitida por el sensor y reflejada por los objetos

\item[•] Radiación terrestre emitida por los objetos 

\item[•] Radiación solar reflejada por los objetos 
\end{itemize}

El grupo de datos adquiridos por medio de los diferentes procedimientos empleados en la teledetección son generados desde plataformas como aviones o satélites, estos contienen por lo general tres tipos de información (espacial, espectral y temporal) \cite{SacristanRomero}.

Una propiedad de los datos obtenidos de la teledetección, es el poder permitir dar seguimiento al desarrollo de las enormes extensiones forestales, existentes en la superficie de la tierra, al tener una perspectiva general de las causas de los efectos que se producen debido a las grandes catástrofes. Como pueden ser las eternas sequías en las regiones desérticas del Sahara de África, en este caso, es posible ubicar determinados fenómenos generados por la contaminación a gran escala afectando al cielo y el ambiente en el mar.

Mediante la tecnología radar se pretende actualizar el conocimiento de las riquezas naturales nacionales e internacionales, ubicadas en las áreas agrícolas, áreas forestales, áreas hidrológicas, áreas mineras o cualquier áreas que represente una fuente de recursos para la humanidad \cite{Romero2007}.

De tal manera que la vigilancia debe ser constante para el medio ambiente, debido a que esta actividad podrá generar una importante reducción en el impacto de degradación causado por los humanos hasta este momento. Por lo que los datos obtenidos del análisis de teledetección se convierten en una importante fuente de información, siendo un gran papel en el cumplimiento de los objetivos del cuidado del medio ambiente.

En el caso español, considerando que las acciones más imprescindibles deben ser enfocadas en la calidad del agua, debido a que el agua es una gran riqueza para la Península Ibérica, básicamente es necesaria para mantener la vida y el desarrollo del ser humano; otra acción importante sería la detección de incendios.

La implementación de la teledetección impacta de forma positiva en la preservación y el mejoramiento de la calidad del agua de los ríos y embalses, debido a que el uso de estas técnicas permite vigilar su situación para poder actuar y manejar cualquier vertido contaminante que se llegara a producir \cite{Rosello2009}.

Es por ello por lo que las técnicas de radar impactan positivamente en la generación de un pronóstico, obteniendo la detección y generando un monitoreo constante de posibles desastres naturales en las diferentes áreas del planeta; como la instalación de una estación de monitoreo de mareas por el \textit{Centro para Alarmas por Tsunamis en el Pacífico}, conocido como PTWC (por sus siglas en inglés), además del \textit{Servicio Nacional de Estudios Territoriales} (SNET), la cual permite obtener información sobre el nivel del mar y los cambios que se originan en las costas del Pacífico. Es así como la obtención de información podría evitar daños por la generación de un tsunami en la región de este océano que podría afectar a Centroamérica.

Todos los elementos existentes en la naturaleza cuentan con una respuesta espectral denominada \textit{signatura espectral}, esta es capaz de estudiar las variaciones espectrales, las variaciones espaciales y las variaciones temporales de las ondas electromagnéticas, poniendo de manifiesto las relaciones existentes entre las variaciones junto con las características que tienen los diferentes materiales terrestres. 

Es por ello que su objetivo básico se centra en la identificación de los diferentes materiales de la tierra y los diferentes fenómenos que en ella se generan a través de su característica espectral. La teledetección como herramienta ofrece enormes posibilidades para la generación de avances en el conocimiento e interpretaciones de condiciones físicoambientales, constituyendo una fuente de información y desempeñando un papel significativo en el campo del conocimiento geográfico \cite{Botana2019}. 

En este sentido la teledetección se define como el proceso de análisis de la energía reflejada por los objetos, es decir, es una técnica que puede aportar una información muy valiosa para distintos campos de intervención. 

\subsection{Tipos de teledetección}

Se llama \textbf{teledetección pasiva} a la técnica aplicada por los sensores que miden las variaciones de la energía procedente de los objetos sin intervenir en el campo natural. 

Los \textbf{sensores pasivos} o también conocidos como ópticos miden (en unidades de radiancia) la energía electromagnética solar reflejada por la Tierra, y además la radiación térmica emitida por la Tierra en distintas bandas espectrales o en diferentes intervalos del espectro electromagnético. Aquí la cantidad de radiación reflejada va a depender de las condiciones de iluminación presentes. La proporción entre la radiación incidente y reflejada, tienen una magnitud denominada reflectancia (propiedad específica de la superficie reflectante).

Se denomina \textbf{teledetección activa} a aquellos que generan un campo de energía artificial (su propia fuente de radiación de microondas), utilizado para registrar y medir el efecto que en él producen los objetos, donde las características técnicas del sensor influyen en la calidad de los datos y  en la posibilidad de recibir información en distintas longitudes de onda \cite{AcevoHerrera2011Apr}. 

Los \textbf{sensores activos} sin imagen son los que incluyen los altímetros y escatómetros, por los cuales en la mayoría de los casos son dispositivos de perfil (toman medidas en una dimensión), es lo opuesto a la representación bidimensional de los sensores de imagen. Los sensores en la teledetección activa se pueden dividir principalmente en dos categorías, en \textit{con imagen} y en \textit{sin imagen}. Debido a que los sensores activos con imagen son basados en técnicas de radar o de SAR para poder generar imágenes en dos dimensiones.

\section{Radar \textit{Acconeer}}

El radar utilizado en el proyecto está fabricado por \textit{Acconeer} y su modelo tiene la referencia \textit{A111}. 

El radar \textit{A111} es un sistema de radar basado en tecnología de radar coherente pulsado, también conocido como PCR\footnote{Pulsed Coherent Radar}, el cual está estableciendo un nuevo punto de referencia para el consumo de energía y la precisión de la distancia, es totalmente integrado en un paquete pequeño de 29 mm\textsuperscript{2} \cite{Acconeer2021}.

El sistema de radar \textit{A111} es de 60 GHz, el cual está optimizado para operar a una alta precisión, con una potencia ultra baja. Por lo general se suministra como una óptima solución de un solo paquete con banda base integrada, interfaz de Radio Frecuencia (RF) y Antena en el Paquete (AiP), esto permitirá una fácil integración en cualquier dispositivo portátil que funcione con batería.

El radar \textit{A111} se basa en una tecnología de sensor patentada de vanguardia con resolución de tiempo de pico de segundo, es capaz de medir distancias absolutas con precisión milimétrica hasta un rango de 2 m, ya que el rango de 2 m está garantizado para un tamaño de objeto, forma y propiedades dieléctricas correspondientes a un reflector esférico de esquina de 5 cm de radio, además de contar con una tasa de actualización configurable.

El radar de 60 GHz no se ve afectado por ninguna fuente natural de interferencia, como lo es el ruido, el polvo, el color o la presencia de luz directa o indirecta.

\imagen{radar}{Radar \textit{A111}.}

Aplicaciones:
\begin{itemize}
	\item[•] Puede generar mediciones de distancia de alta precisión (del orden del mm) y alta tasa de actualización.
	\item[•] Puede realizar detección de proximidad con alta precisión y posibilidad de definir múltiples zonas de proximidad.
	\item[•] Puede generar detección de movimiento y detección de velocidad.
	\item[•] El radar permite la detección de material.
	\item[•] Se puede generar el seguimiento de objetos de alta precisión, que permite el control por gestos.
	\item[•] Es posible desarrollar seguimiento de alta precisión de objetos 3D.
	\item[•] Es posible que controle los signos vitales de la vida, como la respiración y el pulso.
\end{itemize}

Algunas de las características principales con las que se cuenta y que se pueden describir del radar son:

\begin{itemize}
    \item[•] Cuenta con un alcance y movimientos precisos de la distancia, ya que puede:
    \begin{itemize}
        \item Medir el rango absoluto hasta 2 m o precisión absoluta en mm.
		\item Tiene una precisión relativa en micrómetros.
		\item Cuenta con la posibilidad de reconocer movimientos y gestos para varios objetos.
		\item Admite modo de barrido continuo y único.
		\item Tienen ancho de haz de media potencia (HPBW\footnote{Ancho angular total entre los dos puntos que están 3dB abajo del pico del haz principal.}) de 80 (plano H) y 40 grados (plano E).
    \end{itemize}
    \item[•] Su fácil integración es debida a que cuenta con:
    \begin{itemize}
        \item Una solución de un chip con banda base y RF integrados.
		\item Puede integrarse detrás de plástico o vidrio sin necesidad de una apertura física.
		\item Cuenta con componente refluible único.
		\item Fuente de alimentación única de 1.8 V, habilitada con \textit{Power on Reset} (PoR\footnote{Microcontrolador generador de reinicio de encendido.}).
		\item Una entrada de reloj para cristal o reloj de referencia externo, 20-80 MHz.
		\item Una interfaz \textit{Serial Peripheral Interface} (SPI\footnote{Bus de interfaz estándar de comunicaciones, usado principalmente para la transferencia de información entre circuitos integrados en equipos electrónicos.}) para transferencia de datos, soporte de reloj SPI de hasta 50 MHz.
    \end{itemize}
\end{itemize}

El radar \textit{Acconeer A111} es un sensor de radar optimizado de baja potencia y alta precisión de 60 GHz con banda base integrada, una interfaz de RF y una antena en paquete (AiP).

El sensor se basa en la tecnología de radar coherente pulsado (PCR), que presenta una solución patentada de vanguardia con resolución de tiempo de picosegundos. El \textit{A111} es la elección perfecta para implementar sistemas de detección de alta precisión y resolución con bajo consumo de energía.

El radar de silicio \textit{A111} esta dividido en cuatro bloques funcionales: \textit{Power}, \textit{Digital}, \textit{Timing} y radio \textit{mmWave}.

\imagen{figBlockDiagram}{Diagrama de bloques del sensor \textit{A111}.}

La figura \ref{fig:radar} muestra un diagrama de bloques del sensor \textit{A111}. La señal se transmite desde la antena de transmisión (Tx) y es recibida por la antena de recepción (Rx), ambas integradas en la capa superior del sustrato del paquete \textit{A111}. Además de la radio \textit{mmWave}\footnote{Rango de Frecuencia 2 (FR2)}, el sensor consta de administración de energía y control digital, cuantificación de señales, memoria y un circuito de temporización.

El bloque funcional \textit{Power}\footnote{Bloque de conexión eléctrica del radar.} incluye \textit{Low Dropout} (LDO\footnote{Reguladores de voltaje de caída baja.}) y un bloque Power on Reset (PoR). Cada LDO crea su propio dominio de voltaje. El bloque PoR genera una señal de reinicio en cada ciclo de encendido. El host conecta el bloque funcional de alimentación del sensor a través de una fuente de alimentación única de 1,8 V.

El bloque funcional digital incluye control por sensor. La memoria de datos almacena los datos de barrido del radar del conversor de señal analógica a digital (ADC). El host interactúa con el sensor a través de una interfaz SPI, un reloj (XIN, XOUT) y una señal de INTERRUPCIÓN\footnote{Señal recibida por el procesador de una computadora, que indica que debe interrumpir el curso de ejecución actual. }.

El bloque de temporización incluye los circuitos de temporización.

El bloque funcional de radio \textit{mmWave} genera y recibe pulsos de radar e incluye transmisor (TX), receptor (RX) e interfaces hacia las antenas integradas.

El software \textit{Acconeer} se ha escrito en lenguaje \textit{C} y es portátil para cualquier sistema operativo y plataforma \textit{HardWare} (HW\footnote{Equipo o soporte físico}). El software \textit{Acconeer} se ejecuta en \textit{Host Unidad de Control Multipunto} (MCU\footnote{Dispositivo de red que se usa como puente en conexiones de audioconferencia y videoconferencia}) y se entrega como binarios, a excepción del software de integración que se entrega como código fuente.

El RSS (software del sistema de radar) proporciona salida en dos niveles diferentes, servicio y detector. RSS proporciona una API (interfaz de programación de aplicaciones) para la utilización de aplicaciones de varios servicios y detectores.

La salida de servicio son datos de sensor preprocesados en función de la distancia. Por ejemplo, datos de envolvente (amplitud de los datos del sensor), datos de la bandeja de potencia (datos de amplitud integrados en intervalos de rango predefinidos), datos modulados por IQ\footnote{Servicio \textit{In-phase and Quadrature}} (representación en cartesiano), etc. Los detectores se basan en datos de servicio como entrada, y la salida es un resultado. 

El cliente puede utilizar el detector \textit{Acconeer} o desarrollar su propio procesamiento de señales basado en los datos del servicio.
\textit{Acconeer} proporciona varias aplicaciones de ejemplo para respaldar el desarrollo de aplicaciones propias del cliente. Además, se proporcionan pautas para el cliente para el desarrollo de aplicaciones utilizando la API RSS de \textit{Acconeer}.

\textit{Acconeer} proporciona varios controladores de referencia como código fuente, p.ej. soporte para \textit{MCU Cortex M4} y \textit{Cortex M7}.
El software de integración implementará las funciones definidas en un archivo de definiciones proporcionado en la oferta de \textit{Acconeer Software}. Esto incluye el manejo de \textit{SPI, ENABLE e INTERRUPT}, así como posibles funciones del sistema operativo.

En el siguiente enlace se puede acceder a los productos y características que ofrece \textit{Acconeer} (\url{https://www.acconeer.com/products}).

El sensor se puede ejecutar en uno de los siguientes servicios básicos de la tabla \ref{tab:serviciosa111}.

\begin{table}[h]
\begin{center}
\begin{tabular}{| l | c | p{7cm} |}
\hline
\textbf{Servicio} & \textbf{Tipo de dato} & \textbf{Ejemplo de uso} \\ \hline
Envelope & Amplitud & Distancia absoluta y presencia estática \\ \hline
IQ & Amplitud y fase & Detección de obstáculos, respiración y distancia relativa \\ \hline
Sparse & Amplitud instantánea & Velocidad, detección de presencia y detección de gestos \\ \hline
\end{tabular}
\caption{Servicios del radar \textit{A111}}
\label{tab:serviciosa111}
\end{center}
\end{table}


\section{Servicio \textit{In-phase and Quadrature}}

El servicio \textit{In-phase and Quadrature} (IQ\footnote{En fase y cuadratura.}) utiliza la coherencia de fase\footnote{Se dice que dos puntos de una onda son coherentes cuando guardan una relación de fase constante.} del radar pulsado \textit{Acconeer} para producir componentes estables en fase y en cuadratura. Este servicio se puede utilizar para la detección de presencia frente al sensor, la detección de la frecuencia respiratoria, la detección de obstáculos y, en nuestro caso, para diferenciar materiales.

Los componentes en fase y en cuadratura se representan como números complejos, lo que genera un conjunto complejo de \textit{N\textsubscript{D}} muestras representadas como \textit{x[d]}, dónde \textit{d} es el índice de demora de la muestra.

Los datos obtenidos a través del servicio IQ proporcionan un método para examinar la reflectividad a diferentes distancias del sensor de radar.

Las técnicas de análisis de imágenes se han venido desarrollando debido al gran número de aplicaciones donde se emplean, detección y análisis de cambios sobre la superficie terrestre, mediciones de campos, determinación de extensiones de hielo, glaciares, etc.

Es posible el aplicar algoritmos en los que mediante la definición de umbrales adecuados se diferencian las regiones, estableciéndose la presencia de bordes, por ejemplo, usando la diferencia entre píxeles, o en el cociente entre los valores de los píxeles \cite{Varela2019}.

El problema que se presenta en este método es la aparición de un ruido multiplicativo conocido como \textit{speckle}, en donde este ruido se debe a la interferencia que se produce al sumar las señales coherentes provenientes de los elementos dispersores presentes en la región asociada al píxel considerado. Por lo que se hace necesario el uso de herramientas matemáticas más sofisticadas para la detección de bordes, donde las imágenes SAR son ofrecidas en distintos formatos y niveles de procesamiento, siendo la más básica de las imágenes la denominada SLC (\textit{Single Look Complex}). 

Una adquisición SAR en polarización simple, vertical u horizontal, tiene dos bandas de datos que corresponden a los canales de adquisición I (\textit{in phase}) y Q (\textit{in quadrature}), los cuales pueden ser interpretados como la parte real e imaginaria, respectivamente, de una señal compleja.

El \textit{speckle} ha sido descrito teóricamente mediante modelos de distribución en regiones homogéneas, estos modelos establecen que para imágenes SLC de polarización simple (por ejemplo áreas rurales), los datos de ambos canales I y Q, tienen una distribución Gaussiana para las amplitudes. 

Mientras que la fase tiene una distribución prácticamente uniforme y la intensidad de una distribución exponencial (o equivalentemente Gamma de orden 1), el módulo de la amplitud (que es proporcional a la raíz cuadrada de la intensidad) responde a una distribución de Rayleigh. 

Considerando esto, se han usado diferentes técnicas que tienen en cuenta la correspondiente distribución de la potencia que se recibe de cada píxel y se han calculado las distribuciones para los gradientes de intensidad asociados a los píxeles, con esto se ha conseguido una mejor identificación de los bordes.


Un radar \textit{Doppler} tiene etapas de detección síncrona, es decir, que están en sincronía con el oscilador de transmisión, lo que define un receptor coherente, debido a que las señales \textit{Doppler} contienen no sólo información de amplitud sino también de fase, es crucial conocer estas componentes para una correcta demodulación\footnote{Conjunto de técnicas utilizadas para recuperar la información transportada por una onda portadora, que en el extremo transmisor fue modulada con dicha información.}.

Una forma de obtenerlas es indirectamente, conociendo la amplitud de la señal en fase (I) y la amplitud de la señal en cuadratura (Q), donde estas señales se obtienen al mezclar la señal recibida con las señales obtenidas de un oscilador sincronizado que genere una en fase y la otra desfasada 90º \cite{CastilloPlasencia2016Oct}.

Una forma de obtener estas componentes es utilizando receptores enteramente digitales, aunque el receptor analógico tuvo muchos años de vigencia, las técnicas actuales permiten realizar receptores enteramente digitales en un sólo chip.

Es importante notar que la detección de la información se puede realizar en banda base o en una frecuencia intermedia, debido a que la señal de la antena y la señal del oscilador local tendrían la misma frecuencia en el caso del receptor homodino. 

Un receptor digital consiste en una fuente analógica, en este caso la señal recibida por la antena, la cual es previamente filtrada y amplificada por un amplificador de bajo ruido (LNA), para a continuación la señal es digitalizada por un conversor análogo-digital (ADC) y luego es multiplicada digitalmente por una señal de referencia para obtener la frecuencia intermedia o la señal en banda base. 

Además de la señal de entrada RF se obtienen dos señales, la primera es producto de la multiplicación por la referencia, y la otra es producto de la multiplicación por la referencia desfasada 90º, donde el objetivo es obtener información no sólo de magnitud sino también de fase respecto a la referencia.

Cabe señalar que el receptor digital concentra su selectividad en el filtro digital, donde este filtro, para el caso del radar \textit{Doppler}, suele ser un filtro adaptado (\textit{matched filter}) con el fin de obtener la mejor relación de señal a ruido.


\section{Aprendizaje automático}

El aprendizaje automático o \textit{machine learning} es una disciplina del campo de la Inteligencia Artificial (IA),
consiste en dotar a una computadora de capacidad para que mejore en la realización de una tarea a partir de datos de ejemplo o de la experiencia. Se puede visualizar en áreas como los procesos estadísticos, ya que tiene muchas ventajas y reduce el tiempo de demora de los procesos, además de emplear menos personal para realizar las tareas.

Es un área que estudia cómo construir programas de computadoras que mejoren su desempeño en alguna tarea gracias a la experiencia, debido a que esta se basa en las ideas de diversas disciplinas, como inteligencia artificial, estadística y probabilidad, teoría de la información, psicología y neurobiología, teoría de control y complejidad computacional. Estudia la construcción de sistemas capaces de aprender a partir de datos, lo que incluye una gran variedad de sistemas, desde sistemas de visión por ordenador hasta sistemas para detectar correo no deseado (\textit{spam}).

Este sistema proporciona una gran ventaja a la hora de examinar de manera más efectiva una mayor cantidad de datos, debido a que en la actualidad los avances tecnológicos han permitido crear nuevas experiencias tecnológicas en diferentes áreas tales como: empresas, educación, sociedad, entre otros.

La inteligencia artificial es un concepto de creación de máquinas inteligentes que estimula el comportamiento humano, mientras que el aprendizaje automático es un subconjunto de la inteligencia artificial que permite que la máquina aprenda de los datos sin ser programada.

La diferencia entre el software informático normal y el aprendizaje automático es que un desarrollador humano no ha dado códigos que le indiquen al sistema cómo reaccionar ante la situación, sino que está siendo entrenado por una gran cantidad de datos.

La mayoría de información generada a diario en Internet, ya sea por medio de las redes sociales, por medio de diferentes transacciones comerciales o datos generados por distintos dispositivos, en lugar de almacenarla y que solo ocupe espacio en los servidores, es aprovechada por procesos que emplean los datos para poder generar un análisis que establezca comportamientos capaces de identificar las tendencias futuras \cite{Sandoval2018}.

En ciertas ocasiones se reúne demasiada información, por lo que es posible determinar con anticipación y de forma segura el comportamiento futuro de un grupo de personas que interactúan con equipos electrónicos, ya que la técnica del \textit{Machine Learning} (elemento fundamental de la Ciencia de Datos) utiliza métodos para realizar las predicciones de datos y su presentación \cite{CornejoMacias2021}.

Los diferentes dispositivos que cuentan con la conocida inteligencia artificial, son capaces de ejecutar distintos procedimientos análogos con el comportamiento humano. Es el caso de una devolución a una respuesta por cada petición de entrada, parecido o similar a un tipo de reflejos que se encuentran en los seres vivos, estableciendo un estado especifico entre los diferentes estados posibles según una determinada acción, con la solución de problemas empleando una lógica formal. 

Cuando se les es otorgada a un determinado tipo de dispositivos la habilidad de poder aprender, se genera la posibilidad de que puedan discernir. Por lo tanto, se convierten en entidades que razonan con capacidades semejantes a las de un superhombre, dado que su velocidad de procesamiento es prácticamente imposible para un humano promedio. Las maquinas no tienen la necesidad de descansar para poder desarrollar sus funciones correctamente. Son algunas ventajas que ubican a las maquinas sobre los seres vivos en este contexto.

En este sentido es posible encontrar tres grupos de algoritmos en \textit{Machine Learning}.

\begin{itemize}
\item \textbf{Los algoritmos supervisados}, algoritmos que emplean un conjunto de datos de entrenamiento etiquetados o también conocidos como preclasificados. Los datos son procesados para poder realizar predicciones sobre ellos, existiendo la posibilidad de corrección cuando son incorrectas \cite{RussoC2016}. 

\item \textbf{Algoritmos semi-supervisados} combinan tanto datos etiquetados como datos no etiquetados para poder generar una función determinada o clasificada. Estos tipos de modelos son los que tienen que aprender las estructuras para poder organizar los datos, es así como pueden realizar predicciones.

\item \textbf{Los algoritmos no supervisados} están diseñados para clasificar datos de los que no existe información sobre su etiqueta, por lo que no se cuenta con un resultado conocido. Por ello es necesario deducir las estructuras que se encuentran presentes en los datos de entrada, lo cual puede ser conseguido a través de un proceso matemático para poder bajar la redundancia sistemáticamente tratando de organizar los datos por similitud.
\end{itemize}

Dentro de esta clasificación se pueden encontrar diferentes números de algoritmos específicos con las diferentes características para poder generar el tratamiento de los datos, entre los algoritmos más relevantes podemos encontrar el \textit{Deep Learning}, consiste en el empleo de algoritmos para poder hacer representaciones abstractas sobre la información y facilitar el aprendizaje automático. 

Otro algoritmo es \textit{Active Learning}, es un caso de aprendizaje semi-supervisado, el algoritmo de aprendizaje puede interactuar con un usuario, además de otra fuente de información para poder obtener los resultados deseados.

El clasificador \textit{Support Vector Machines (SVM)} busca generar la maximización de la distancia de las muestras de cada clase a la llamada «frontera de decisión» (también conocida como plano). En el caso de que las muestras obtenidas no sean linealmente separables se emplea una transformación llamada \textit{kernel}.

En todos los casos, un sistema que aprende debe ser capaz de generalizar, es decir, de encontrar patrones y regularidades en los datos que le permitan desempeñarse bien en datos que no ha observado previamente.


\subsection{Tipos de aprendizaje automático}

\subsubsection{Aprendizaje supervisado}

El algoritmo de aprendizaje conocido como supervisado emplea un programa capaz de recibir datos de entrada etiquetados y los datos de salida esperados. Obtiene los datos de los datos de entrenamiento que contienen conjuntos de ejemplos.

Generan dos tipos de resultados:
\begin{itemize}
\item[•] Clasificación: notifican la clase de los datos que se presentan.
\item[•] Regresión: esperan que el producto produzca un valor numérico.
\end{itemize}

El aprendizaje supervisado, se da cuando se entrena un algoritmo de \textit{Machine Learning} aportándole las preguntas, las cuales son características, y una serie de respuestas conocidas también como etiquetas, por lo que así en el futuro el algoritmo sea capaz de hacer una predicción bajo el conocimiento de las características. Generalmente en este tipo de aprendizaje hay dos algoritmos, son el algoritmo de clasificación y el algoritmo de regresión.

Los algoritmos de clasificación son los que esperamos que por su naturaleza nos indiquen a qué grupo pertenece un determinado elemento en estudio. Por lo general, el algoritmo tiene la capacidad de encontrar patrones en los datos que le proporcionamos pudiendo clasificarlos en grupos, ya que luego se comparan con nuevos datos obtenidos, para después poder ubicarlos en uno de los grupos, siendo de esta manera como puede predecir de que se trata de objeto de estudio \cite{gonzalez2015}. 
 
\subsubsection{Aprendizaje no supervisado}

El aprendizaje sin supervisión es un conjunto de metodologías que por lo general se emplean principalmente en análisis exploratorios, debido a que puede identificar automáticamente la estructura en los datos.

En el caso de los algoritmos de aprendizaje no supervisado, no es necesaria de la intervención humana para poder desarrollar un conjunto de datos, los cuales son previamente clasificados para poder representar el algoritmo de aprendizaje, dado que el objetivo de la enseñanza no supervisada es el poder encontrar modelos interesantes teniendo en cuenta la distribución con la compilación de los datos que se presentan. Algunos ejemplos que se pueden mencionar sobre técnicas de aprendizaje no supervisado son las técnicas que se aplican de agrupación \cite{GonzalezPerez2020}.

\subsubsection{Aprendizaje semisupervisado}

El algoritmo de aprendizaje semisupervisado emplea el método del aprendizaje que es automático, el cual evita la gran cantidad de datos etiquetados, mejorando este tipo de limitación, ya que aprende a clasificar a partir de un número, casos de ejemplos etiquetados junto con otros no etiquetados. Los datos etiquetados se usan para poder aprender modelos que caractericen cada clase o categoría, y los ejemplos sin etiqueta se usan para refinar los límites entre las clases \cite{Cardoso2020Apr}.

El llamado \textit{self-training} la cual es una de las formas más simples de clasificación semisupervisada, donde primero se construye un clasificador usando los datos etiquetados, para después emplear el clasificador que categoriza a los datos no etiquetados, ya que sólo los nuevos ejemplos etiquetados con una confianza que supere cierto umbral se anexan al conjunto etiquetado, es cuando el proceso de aprendizaje se repite.

Por obvias razones los algoritmos de aprendizaje semisupervisado se encuentran entre los de aprendizaje supervisado y los de aprendizaje sin supervisión, dado que en los algoritmos de aprendizaje semisupervisado es posible dividir los datos en dos partes, las cuales están integradas por un grupo de datos clasificados y por un grupo de datos no clasificados, por lo consiguiente se llama aprendizaje semisupervisado estándar.

Para diferentes investigadores, el aprendizaje semisupervisado es más útil cuando hay más datos no clasificados comparado con los datos clasificados, dado que especialmente cuando es necesario mucho esfuerzo para poder obtener
datos clasificados se tiende a tardar tiempo aumentando el costo, y obtener datos no clasificados generalmente es más barato.

Algunos ejemplos donde se aplican las técnicas de aprendizaje semisupervisado es en las técnicas de máquinas de vectores de soporte transductivo, otro ejemplo es la maximización de las expectativas, además de poder mencionar algunas aplicaciones donde se aplican las técnicas de aprendizaje semisupervisado mencionadas, estas podrían ser la clasificación de páginas web, el reconocimiento de voz y la secuencia de la proteína.


\subsubsection{Aprendizaje por refuerzo}

El aprendizaje por refuerzo es un campo dentro del aprendizaje automático, los problemas que se tratan en esta área requieren de la toma de un conjunto de decisiones secuenciales para poder conseguir un objetivo determinado, dado que, en estas situaciones, una entidad denominada \textit{agente} es la que interacciona con el entorno, debido a que aprende de forma autónoma las acciones que debe llevar a cabo con el fin de maximizar una señal numérica escalar, conocida como recompensa \cite{GuerraRamos2020}.

El algoritmo de aprendizaje por refuerzo puede aprender observando el mundo que lo rodea, la información de entrada es la retroalimentación que recibe del mundo exterior en respuesta a sus acciones, por lo que el sistema aprende sobre la base de pruebas y errores. El algoritmo interactúa con el entorno y por lo general se puede plantear un método de seguimiento, donde es posible encontrar un error lo suficientemente apto para encontrar el mejor resultado basado en la experiencia. Por lo general, en un principio el agente no conoce la dinámica del entorno en el que se encuentra, tampoco conoce los detalles explícitos de la tarea que debe realizar, por lo que la experimentación es la única forma de la que se dispone para poder desarrollar el entrenamiento.

Dado que las dos cualidades implementadas, la experimentación y la recompensa con retraso, forman parte esencial de los problemas de aprendizaje reforzado, son estas las cualidades distintivas las que no pertenecen a ningún otro campo del aprendizaje automático.

Se puede decir que el objetivo de este aprendizaje es el poder aprender la función de valor, la cual ayuda al agente inteligente para poder maximizar la señal de recompensa, buscando optimizar sus políticas para comprender el comportamiento del entorno y así tomar las decisiones correctas, logrando cumplir con sus objetivos formales. 

Podemos decir que entre las implementaciones desarrolladas se encuentra \textit{AlphaGo}, programa de Inteligencia Artificial, desarrollado por \textit{Google DeepMind} para poder jugar el juego de mesa \textit{Go}, podría resultar interesante mencionar que en marzo de 2016, \textit{AlphaGo} ganó el partido ante el jugador profesional Lee Se-Dol, el cual tiene la categoría novena «Dan» con 18 títulos mundiales. Uno de los algoritmos utilizados es el «árbol de búsqueda de Monte Carlo»\footnote{Algoritmo de búsqueda heurístico realiza un proceso de toma de decisiones sobre todo relativo con los juegos.} que utiliza el aprendizaje profundo con redes neuronales.

\subsubsection{Aprendizaje multitarea}

El aprendizaje multitarea, también conocido como \textit{Multitask Learning} (MTL), esta basado en la resolución de varias tareas al mismo tiempo. Este aprendizaje aprende tanto de las características comunes como de las diferencias entre ellas, por lo cual, el modelo estudia en paralelo todas las tareas, al mismo tiempo comparte la información aprendida con cada una de las tareas para poder resolver las otras  tareas, por lo general esta técnica es especialmente útil cuando las tareas están relacionadas. Se ha demostrado que puede ser muy poderoso para tareas no relacionadas \cite{CerdaMunoz2021Feb}.

El aprendizaje multitarea es la aplicación de los métodos de aprendizaje que emplean el conocimiento previamente aprendido por el sistema, se emplean cuando se enfrentan a problemas similares a los ya vistos, lo que implica generar una solución simultánea de diferentes tareas. El aprendizaje de una tarea se mejora para poder complementarla con el aprendizaje común de otras tareas relacionadas con esa tarea.

Unas de las ventajas más importantes al emplear aprendizaje multitarea son las que comprenden la automatización, la precisión, la personalización, la rapidez, y la escalabilidad.


\section{Algoritmos \textit{machine learning} considerados}

\textit{Machine Learning}, en castellano aprendizaje automático, es una disciplina científica que se enfoca en crear sistemas que aprenden de datos, los cuales son capaces de realizar predicciones e identificar patrones complejos en ellos. Se puede establecer que por definición estos sistemas tienden a mejorar de forma autónoma, el científico informático Tom Mitchell establece una definición moderna de lo que es \textit{Machine Learning}, \textit{«Se dice que un programa de ordenador aprende de la experiencia E con respecto a alguna clase de tareas T y la medida de rendimiento P, si su desempeño en tareas en T, medido por P, mejora con la experiencia E.»}, dada esta definición es posible identificar dos tipos de aprendizajes, estos son el aprendizaje supervisado y aprendizaje no supervisado \cite{RodriguezGonzalez2018}. 

La principal diferencia entre ambos está en el conjunto de datos que usaremos para entrenar, si usamos datos etiquetados o con un valor real estaremos hablando de aprendizaje supervisado, mientras que, si el conjunto de datos no está etiquetado, nuestro algoritmo deberá buscar un patrón en estos, por lo que estaremos ante un problema de aprendizaje no supervisado.

Los lenguajes de programación más usados en \textit{Machine Learning} son: \textit{Python, R, C++, Java y Scala}. \textit{R} se usa debido a que es un lenguaje muy orientado al análisis estadístico y cuenta con muchas librerías desarrolladas capaces de analizar un conjunto de datos. \textit{Python} destaca por su sintaxis intuitiva y una gran variedad de librerías para el desarrollo de aplicaciones de \textit{Machine Learning}, las más relevantes son: 

\begin{itemize}
\item[•] \textit{Pandas}: Nos ofrece una forma versátil de recoger estos datos en \textit{streaming} para poder procesarlos posteriormente en \textit{dataframes}.
\item[•] \textit{SciPy}: Herramientas de matemáticas, ciencia e ingeniería. 
\item[•] \textit{Statsmodel}: Nos provee herramientas dedicadas para el análisis de series temporales.
\item[•] \textit{Scikit-learn}: Para algoritmos de \textit{Machine Learning}. 
\item[•] \textit{Numpy}: Para vectores y matrices.
\item[•] \textit{Keras}: Al igual que \textit{TensorFlow}, es una librería destinada al desarrollo de redes neuronales.
\end{itemize}

Los algoritmos que se han propuesto para ser utilizados en el planteamiento del desarrollo del proyecto son \textit{Random Forest}, \textit{k-NN}, \textit{SVM}, \textit{Auto-Sklearn} y \textit{TabPFN} los cuales se describirán a continuación.

\subsection{\textit{Random Forest}}
\textit{Ramdom Forest} conocido en castellano como Bosques Aleatorios, es un algoritmo de clasificación supervisado, consiste en crear muchos árboles para luego usarlos en la predicción de la variable de interés. Para poder establecer un algoritmo basado en \textit{Random Forest} (bosques aleatorios), es necesario describir en qué consiste un árbol de decisión, ya que en él podemos encontrar la combinación de los diferentes casos.

El árbol de decisión es una de las herramientas de aprendizaje supervisado más implementadas, este modelo consta de una serie de nodos, en los cuales se toman decisiones lógicas de forma secuencial. Por ejemplo, si quisiéramos conocer si a alguien le deberíamos conceder un crédito, podríamos ver si en primer lugar obtiene más o menos de \textit{X} ingresos \cite{AndresAlbelda}. En caso que fuera afirmativo podríamos querer saber si lleva más de \textit{Y} años en su trabajo, o si tiene algún aval, dado que un árbol de decisión no es más que una acumulación de este tipo de decisiones. Empezamos en el nodo conocido como raíz, pasamos por los nodos intermedios y nuestro resultado es proporcionado por un nodo terminal, que serían las hojas del árbol.

Al final, los árboles de decisión lo que hacen es atribuir un valor constante al \textit{output}\footnote{Resultado de procesar un conjunto de datos.} dentro de regiones que suelen ser rectángulos definidos en el espacio de los \textit{inputs}\footnote{Conjunto de datos que se introducen en el sistema.}. La ventaja principal de estos métodos en la sencillez de interpretación, ya que podemos encontrar un caso con dos variables, donde el dominio de la variable \textit{X1} se ha dividido en tres regiones, mientras que el de \textit{X2} se descompone en función del valor de la primera variable.

Los árboles se crean siguiendo el algoritmo:

\imagen{randomforest}{\textit{Random Forest}}

\begin{itemize}
\item[•] Sea \textit{N} el número de casos de prueba, \textit{M} es el número de variables en el clasificador.
\item[•] Sea \textit{m} el número de variables de entrada, \textit{m} menor que \textit{M}
\item[•] Se elige un conjunto de datos para el entrenamiento del árbol y el resto de los casos se utilizará para estimar el error.
\item[•] Para cada nodo del árbol, es posible elegir aleatoriamente \textit{m} variables en las cuales basar la decisión, y así poder calcular la mejor partición del conjunto de entrenamiento a partir de las \textit{m} variables.
\end{itemize}

La idea es que a medida que se vaya separando la muestra de entrenamiento a través de los diferentes nodos, se vayan formando grupos cada vez más homogéneos, hasta que las decisiones no generen grupos más uniformes. El método fragmenta el espacio formado por los datos obtenidos asignando un valor como resultado para todos los subespacios.


\subsection{\textit{k-NN}}

\textit{k-NN} (k vecinos más cercanos) también conocido como \textit{k-nearest neighbors}, es un algoritmo de tipo simple, se basa en un control de parecido. \textit{k-NN} se emplea para obtener el reconocimiento de patrones, también es empleado para obtener la estimación estadística, ya que es una técnica no paramétrica, y no hace suposiciones con los datos subyacentes. Su funcionalidad es poder buscar un número anteriormente definido de un conjunto de datos de entrenamiento que esté cercano al nuevo punto, pudiendo predecir la etiqueta a partir de los datos.

\imagen{KNN}{\textit{k-nearest neighbors}}

La cantidad de datos son una constante definida por los usuarios conocida como aprendizaje del vecino más cercano \textit{K}, es posible que varíe de acuerdo con la cantidad local de puntos, donde la distancia es cualquier medida, por lo que la distancia euclidiana estándar es la más empleada. Dado que el dato más cerca de \textit{K} es un algoritmo de aprendizaje automático basado en técnicas de aprendizaje supervisado, se asume el parecido entre el nuevo caso con los datos, ya que los casos disponibles para poder establecer el nuevo caso en su categoría deberá ser más similar a las categorías existentes \cite{JimenezLeon2021}.

El algoritmo \textit{k-NN} es capaz de almacenar los datos disponibles, clasificando un punto en función de la similitud, por lo que cuando aparecen datos nuevos, es posible clasificarlos fácilmente en una categoría empleando el algoritmo \textit{k-NN}. Puede ser conocido como algoritmo de aprendizaje perezoso debido a que almacena el conjunto de datos. 

Los algoritmos \textit{k-NN} cuentan con dos parámetros, la \emph{K} que representa el número de vecinos y la \textit{k} que es un parámetro de suavizado, donde mientras más grande sea \textit{k} más pequeño es el ruido, y \textit{d} es a cantidad de retardos para cada elemento. 

\subsection{\textit{SVM}}

\textit{Support Vector Machine} o \textit{SVM} es uno de los algoritmos de aprendizaje supervisado más populares, que se utiliza tanto para problemas de clasificación como de regresión. Sin embargo, principalmente, se utiliza para problemas de clasificación en aprendizaje automático.

El objetivo del algoritmo es encontrar un hiperplano en un espacio N-dimensional que clasifique claramente los puntos de datos. Para ello \textit{Scikit-learn} tiene una funcionalidad integrada, el método \textit{GridSearchCV} encargado de seleccionar hiperparámetros. Los hiperparámetros son variables que rigen el proceso de entrenamiento de un modelo, como el tamaño del lote o la cantidad de capas ocultas de una red neuronal profunda. 

\textit{GridSearchCV} se encarga de buscar hiperparámetros óptimos y, por lo tanto, mejorar los resultados de precisión/predicción. \cite{BibEntry2022Aug}

Ventajas del algoritmo:
\begin{itemize}
	\item[•]Eficaz en espacios de altas dimensiones.
	\item[•]Efectivo en casos donde el número de dimensiones es mayor que el número de muestras.
	\item[•]Utiliza un subconjunto de puntos de entrenamiento en la función de decisión (llamados vectores de soporte), por lo que también es eficiente en memoria.
\end{itemize}

\subsection{\textit{auto-sklearn}}

\textit{Auto-sklearn} es un conjunto de herramientas de \textit{Machine Learning} de código abierto, además de ser un reemplazo directo para los clasificadores de \textit{scikit-learn}.\cite{auto-sklearn} 

Podemos decir que se libera al usuario de elegir los algoritmos y el ajuste de los hiperparámetros, ya que proporciona aprendizaje automático supervisado basándose en la optimización bayesiana, el meta-aprendizaje y la construcción de conjuntos.

\textit{Auto-sklearn} envuelve un total de 15 algoritmos de clasificación, 14 algoritmos de preprocesamiento de funciones y se ocupa del escalado de datos, la codificación de parámetros categóricos y los valores faltantes. Se puede ampliar fácilmente con nuevos métodos de clasificación, regresión y preprocesamiento de funciones. Para hacerlo, un usuario debe implementar una clase contenedora y registrarla para \textit{auto-sklearn}.

\subsection{\textit{TabPFN}}

Se ha decidido incluir en el proyecto un nuevo método clasificador llamado \textit{TabPFN}. Es un transformador\footnote{Tipo de algoritmo de Deep Learning}\cite{Lopez2020Feb} capacitado que puede realizar una clasificación supervisada para pequeños conjuntos de datos en menos de un segundo, no necesita ajuste de hiperparámetros y es competitivo con los métodos de clasificación más avanzados.

En la web donde se presenta \href{https://www.automl.org/tabpfn-a-transformer-that-solves-small-tabular-classification-problems-in-a-second/}{\textit{TabPFN}} se indica que el método supera claramente a los árboles potenciados con un aumento de velocidad de hasta 70 veces.

Este método al ser bastante novedoso por ahora limita los problemas hasta 1000 ejemplos de capacitación, 100 funciones y 10 clases. En un futuro el clasificador irá aumentando las limitaciones que presenta a día de hoy.

\section{Proceso de lectura y procesamiento de la señal del lector}

Se ha realizado la extracción de características de los objetos utilizados para el entrenamiento de los modelos clasificadores del presente proyecto. Las características y demás atributos nos darán la posibilidad de identificar los distintos materiales por los que están compuestos los objetos. Estos datos están representados por números complejos, por lo que serán extraídos de las lecturas y separados en módulo y fase para poder realizar cálculos correctos.

Inicialmente se realizaron pruebas de extracción desde la herramienta facilitada por \textit{Acconeer}, figura \ref{fig:gui_acconeer}.

\imagen{gui_acconeer}{Herramienta \textit{Acconeer}}

Ejemplo de como se muestra una lectura en la interfaz, figura \ref{fig:lectura_acconeer}.

\imagen{lectura_acconeer}{Lectura realizada en la herramienta de \textit{Acconeer}.}

La aplicación \textit{Acconeer Exploration Tool} se puede descargar desde el repositorio de este proyecto (\href{https://github.com/mecyc/TFG_RADAR_60GHZ/tree/main/acconeer-python-exploration}{\textit{acconeer-python-exploration}}). En esta interfaz se conecta utilizando el modo \textit{socket} a la IP de la \textit{Raspberry Pi 4}.
 
Una vez ejecutada la aplicación hay cuatro servicios y varias funcionalidades. Los servicios son:
\begin{itemize}
\item[•] \textit{Power Bins}: según \url{https://acconeer-python-exploration.readthedocs.io/en/latest/services/pb.html}, calcula la energía de diferentes distancias, su objetivo es la medición de objetos grandes a distancias cortas como un sensor de parking. Al ser un método muy sencillo con poca cantidad de datos y enfocado a grandes objetos como una pared no se ha contemplado para el estudio.
\item[•] \textit{Envelope}: según \url{https://acconeer-python-exploration.readthedocs.io/en/latest/services/envelope.html}, es igual que Power Bins pero utilizando un espectro continuo de los datos. Su caso de uso típico es la detección estática, por esto se ha contemplado para el estudio ya que la basura estará quieta.
\item[•] \textit{IQ}: según \url{https://acconeer-python-exploration.readthedocs.io/en/latest/services/iq.html}, utiliza la coherencia de fase del radar que detecta movimiento a nivel fino. Tiene cinco modos, el primero es el que ha sido relevante al detectar mejores reflejos de los datos y estar optimizado para distancias muy cortas.
\item[•] \textit{Sparce}: según \url{https://acconeer-python-exploration.readthedocs.io/en/latest/services/sparse.html}, se basa en la señal está más cuantizada y su uso principal es el análisis del movimiento y no de situaciones estáticas, por ello no se ha contemplado su uso para el estudio.
\end{itemize}

Para la creación del modelo se han seleccionado 30 materiales divididos en, 10 de plástico, 10 de cristal y 10 de cartón. De cada material se han realizado 10 lecturas, de varias caras, girando el objeto.

Para unificar el proyecto y no depender de la herramienta de \textit{Acconeer} se ha decidido realizar la extracción de características de materiales desde un cuaderno de \textit{Jupyter} (\textit{\href{https://github.com/mecyc/TFG_RADAR_60GHZ/blob/main/scripts/LecturasRadar.ipynb}{LecturasRadar.ipynb}}), estableciendo una configuración y un estándar en el radar para un correcto tratamiento de datos.

Se ha establecido el radar con los siguientes parámetros:
\begin{itemize}
	\item[•]Servicio \textit{IQ}
	\item[•]Perfil 2 (distancias de unos 20 cm)
	\item[•]Tasa de lectura de 30 Hz
	\item[•]Normalización desactivada (solo se recomienda activada para representar datos)
	\item[•]Ganancia medio punto, la ganancia es la intensidad de la señal.
\end{itemize}


Llegamos a una colección de 300 lecturas exportadas cada una en ficheros con formato \textit{numpy} (.npy) donde están almacenadas las características en vectores y matrices. Un porcentaje de estos datos conformarán la red de entrenamiento y otro porcentaje servirán para testear la red. En el directorio \textit{GitHub} del proyecto se incluye una carpeta llamada «\href{https://github.com/mecyc/TFG_RADAR_60GHZ/tree/main/Materiales}{Materiales}» que contiene los 300 archivos originales.

Cada instante de tiempo de la lectura comprende 291 atributos, de cada fichero obtenemos del orden de 300 instancias. Una instancia son estadísticas de los 291 atributos calculadas a partir de los 300 instantes de tiempo establecidos.


\section{Transformación de las características}

Una vez extraídas las características de los materiales se han realizado una serie de transformaciones en las lecturas, estas devuelven datos mostrados como números complejos.

Se creó una función que a partir del nombre del fichero \textit{npy} cargue los datos, los redimensione y los devuelva.

Función $get\_data$ devuelve un array de 2 dimensiones (2D) al que llamaremos $datos\_bruto$. 

\imagen{get_data}{Función $get\_data$}


Una función \textit{$get\_modulo\_fase$} que a partir del array anterior, obtenga un array 2D, con el doble de anchura. Por ejemplo, si \textit{$datos\_bruto$} es de $300 x 291$, $modulo\_fase$ será de $600 x 291$. 

Esa función obtendrá el módulo del número complejo y la fase del número complejo. El módulo será una matriz de $300 x 291$ y la fase también. Será necesario concatenarlas «horizontalmente». 

\imagen{get_modulofase}{Función $get\_modulo\_fase$}


A partir de los datos del modulo y la fase se obtiene la media. La comprobación sería que devuelva un array de 1 dimensión de tamaño $582$ $(291 x 2)$.

\imagen{get_media}{Función $get\_media$}

Una vez se han transformado las característica se decide utilizar un método de selección de estas.

\section{Métodos de selección de características}

En esta sección se indican los métodos de selección de características que se han utilizado.

\subsection{Validación cruzada}

La validación cruzada, o \textit{cross validation}, es una técnica para evaluar los resultados de un análisis estadístico y garantizar que son independientes de la partición entre los datos de entrenamiento (train) y prueba (test).

Este método lo utilizaremos para validar y comparar los modelos de aprendizaje automático elegidos.

El procedimiento elegido para crear las particiones de entrenamiento y test como indica la validación cruzada es el siguiente.

Se ha decidido crear una función (\textit{particiones}) encargada de realizar automáticamente las particiones separadas en entrenamiento ($part\_train$) y test ($part\_test$). El desarrollo del código lo podemos encontrar en el fichero \href{https://github.com/mecyc/TFG_RADAR_60GHZ/blob/main/scripts/GenerarParticiones.ipynb}{\textit{GenerarParticiones.ipynb}}.

Podemos decir que se realiza una validación cruzada dejando uno fuera. Se crean diez particiones que se corresponden con las diez vistas de cada material. Por lo que en la partición \textit{N} de entrenamiento se deja fuera la \textit{N} vista de cada material, y estas vistas excluidas para entrenamiento pasarán a la partición de test.

Resumen de la creación de particiones:
\begin{verbatim}

Partición 1
Entrenamiento: [Carton Vista2-Vista10] + [Plastico Vista2-Vista10] 
				+ [Cristal Vista2-Vista10]
Test: [Carton Vista1] + [Plastico Vista1] + [Cristal Vista1]

Partición 2
Entrenamiento: [Carton Vista1,Vista3-Vista10] + [Plastico V1, Vista3-Vista10] 
				+ [Cristal Vista1,Vista3-Vista10]
Test: [Carton Vista2] + [Plastico Vista2] + [Cristal Vista2]

...

Partición 10
Entrenamiento: [Carton Vista1-Vista9] + [Plastico Vista1-Vista9] 
				+ [Cristal Vista1-Vista9]
Test: [Carton Vista10] + [Plastico Vista10] + [Cristal Vista10]

\end{verbatim}

Bucles utilizados para crear las particiones:

\imagen{bucleParticiones}{Bucle función $particiones$}


Una vez tenemos las diez particiones creadas procedemos a entrenar los algoritmos.
Los algoritmos utilizados en el proyecto \textit{Random Forest}, \textit{k-NN}, \textit{SVM} y \textit{auto-sklearn} se han entrenado mediante validación cruzada. Para entrenar \textit{TabPFN} a parte de la validación cruzada se han tenido que reducir los atributos a 100 (número máximo que permite \textit{TabPFN})  mediante \textit{Pipeline} junto con \textit{SelectFromModel}.


\subsection{\textit{Pipeline}}

\textit{Pipeline} se ha empleado para reducir los atributos que se han empleado para entrenar el clasificador \textit{TabPFN}. Este clasificador actualmente limita los problemas hasta 1000 ejemplos de capacitación, 100 funciones y 10 clases. En un futuro el clasificador irá aumentando las limitaciones que presenta a día de hoy. 

\textit{SelectFromModel} se usa para extraer las mejores características de los conjuntos de datos y requiere un estimador, en este caso la clase\textit{LogisticRegression} es el modelo escogido para que \textit{Pipeline} seleccione las 100 características para entrenar a \textit{TabPFN}.

\imagen{pipeline}{Pipeline}

Usamos $threshold=-np.inf$ para que ignore el valor de la importancia de las características y use el número establecido en $max\_features$ (número máximo de características).